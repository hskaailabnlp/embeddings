{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantische Ähnlichkeit von Texten\n",
    "\n",
    "Natürliche Sprachen bieten uns mannigfaltige Möglichkeiten, dieselben Inhalte auf unterschiedliche Art und Weise auszudrücken. Für die Interpretation natürlichsprachlicher Texte reicht es daher nicht, auf den Ebenen von Morphologie und Syntax zu bleiben und die Form von Wörtern sowie deren Anordnung im Satz zu betrachten, sondern wir müssen als zusätzliche Ebene die der Semantik betrachten.\n",
    "\n",
    "Dabei beginnen wir zunächst mit der Betrachtung der Wortebene und sammeln Erfahrung mit word2vec.\n",
    "\n",
    "Anschließend wollen wir versuchen, von Wort- zu Satzbedeutungen zu kommen: Wie gut können wir die Ähnlichkeit zweier Sätze unter Rückgriff auf die semantischen Embeddings der in ihnen enthaltenen Wörter bestimmen?\n",
    "\n",
    "Als Orientierung dient uns dabei das [STS-Benchmark-Datenset](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark), das im Rahmen von [SemEval](https://en.wikipedia.org/wiki/SemEval) entstanden ist.\n",
    "Das Datenset enthält Trainings-, Test- und Evaluationsdaten zu folgender Aufgabenstellung: \n",
    "Gegeben zwei Sätze S1 und S2, berechne Ähnlichkeitswert zwischen 0 (völlig unterschiedlicher Inhalt) und 5 (bedeutungsgleich)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Word2Vec\n",
    "> You shall know a word by the company it keeps.\n",
    ">\n",
    "> -- <cite>J. R. Firth</cite>\n",
    "\n",
    "Im ersten NLP-Labor haben wir mit dem Bag-of-Words-Modell eine recht einfache Methode festgestellt, um Wörter bzw. ganze Sätze in einen Vektorraum abzubilden: Jedem Wort wird ein eindeutiger Index zugewiesen. Es lässt sich dann als one-hot-enkodierter Vektor darstellen, der an eben jenem Index eine Eins und ansonsten nur Nullen enthält und dessen Länge der Anzahl der Wörter im Vokabular entspricht. Die Enkodierung ganzer Sätze erhält man durch Aufsummieren dieser one-hot-enkodierten Vektoren.\n",
    "Bei einem solchen Modell geht auf Satzebene die Information über die Reihenfolge der Wörter verloren und ganz allgemein wird offensichtlich die Semantik der Wörter nicht berücksichtigt.\n",
    "\n",
    "Gehen wir zum Beispiel von folgendem Vokabular aus: ```{\"heiß\", \"warm\", \"Gurke\"}``` und enkodieren die einzelnen Wörter wie folgt: ```[1, 0, 0], [0, 1, 0], [0, 0, 1]```, dann sind sich \"heiß\" und \"warm\" genauso ähnlich oder unähnlich wie \"heiß\" und \"Gurke\".\n",
    "\n",
    "Semantische Embeddings schaffen an dieser Stelle Abhilfe. Ein bekannter Vertreter dieser Ansätze ist das Word2Vec-Modell, das 2013 von [Tomas Mikolov et al.](https://arxiv.org/abs/1301.3781) bei Google entwickelt wurde und seither rasch an Verbreitung gewonnen hat.\n",
    "\n",
    "Wir werden mit vortrainierten Google-News-Embeddings arbeiten, die [hier](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) heruntergeladen werden können. Die Vektoren haben die Länge 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.1: Vorbereitung\n",
    "Ladet die Embeddings über den oben angegebenen Link herunter und verwendet gensim, um sie zu laden.\n",
    "\n",
    "**Hinweis**: Kann einen Moment dauern, bis das Dictionary, das von Wort auf Embedding abbildet, erzeugt ist. Im Zweifel ein ```limit``` angeben und nur die ersten 1,5 Mio. Embeddings laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True, limit=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/lab/lib/python3.6/site-packages/gensim/models/keyedvectors.py:730: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sieht gut aus.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_message = \"Oh, da ist wohl etwas schiefgelaufen! Der folgende Test schlägt fehl: {}.\"\n",
    "    most_similar = embeddings.most_similar(positive=['man', 'queen'], negative=['woman'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'king':\n",
    "        return error_message.format(\"'Most similar'\")\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['spring', 'rain', 'autumn', 'summer'])\n",
    "    if doesnt_match != 'rain':\n",
    "        return error_message.format(\"'Doesn't match'\")\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('school', ['cat', 'sound', 'university', 'whine'])\n",
    "    if most_similar_to_given != 'university':\n",
    "        return error_message.format(\"'Most similar to given'\")\n",
    "    \n",
    "    return \"Sieht gut aus.\"\n",
    "\n",
    "check_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.2: Spaß mit Semantik\n",
    "Im Folgenden wollen wir uns ein bisschen mit dem Mehrwert beschäftigen, den semantische Embeddings bieten.\n",
    "\n",
    "Mit ihnen lassen sich zum Beispiel folgende Fragen beantworten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das deutsche New York ist: Berlin\n",
      "Der Mozart der Naturwissenschaft ist: Gottfried_Wilhelm_Leibniz\n",
      "burning:burnt wie singing:sang\n",
      "Ähnlichkeit DE, FR: 0.6270756125450134\n",
      "ähnlichkeit DE, CAN: 0.3807849586009979\n"
     ]
    }
   ],
   "source": [
    "# Welche Stadt ist das New York Deutschlands? (Hinweis: 'New_York' ist als Token in den Embeddings enthalten)\n",
    "print('Das deutsche New York ist: {}'.format(embeddings.most_similar_cosmul(positive=['New_York', 'Germany'], negative=['USA'])[0][0]))\n",
    "\n",
    "# Wer ist eigentlich der Mozart der Naturwissenschaft?\n",
    "print('Der Mozart der Naturwissenschaft ist: {}'.format(embeddings.most_similar_cosmul(positive=['Mozart', 'science'], negative=['music'])[0][0]))\n",
    "\n",
    "# Welches Wort verhält sich zu 'singing' wie 'burnt' zu 'burning'?\n",
    "print('burning:burnt wie singing:{}'.format(embeddings.most_similar_cosmul(positive=['singing', 'burnt'], negative=['burning'])[0][0]))\n",
    "\n",
    "# Sind sich Deutschland und Frankreich ähnlicher oder Deutschland und Kanada?\n",
    "print('Ähnlichkeit DE, FR: {}'.format(embeddings.similarity('Germany', 'France')))\n",
    "print('ähnlichkeit DE, CAN: {}'.format(embeddings.similarity('Germany', 'Canada')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Interpretation der Ergebnisse ist jedoch Vorsicht geboten: Es werden zwar semantische Beziehungen abgebildet, aber die entsprechen möglicherweise nicht immer den Erwartungen.\n",
    "\n",
    "Wie ähnlich sind sich zum Beispiel \"Leben\" und \"Tod\", \"kalt\" und \"warm\", \"Norden\" und \"Süden\"?\n",
    "\n",
    "Sind die Ergebnisse wie erwartet? Warum (nicht)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36187765\n",
      "0.59530354\n",
      "0.7784994\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.similarity(\"life\", \"death\"))\n",
    "print(embeddings.similarity(\"cold\", \"warm\"))\n",
    "print(embeddings.similarity(\"North\", \"South\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Embeddings sind nicht neutral, sondern spiegeln die Beziehungen wieder, die sich in den Trainingsdaten finden lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wissenschaft wird gemacht von: man\n",
      "Mörder sind: black\n",
      "Mann ohne Intelligenz: woman\n"
     ]
    }
   ],
   "source": [
    "# Wird Wissenschaft von Frauen oder Männern gemacht?\n",
    "print('Wissenschaft wird gemacht von: {}'.format(embeddings.most_similar_to_given('scientist', ['man', 'woman'])))\n",
    "# Sind Mörder eher Schwarze, Weiße oder Asiaten?\n",
    "print('Mörder sind: {}'.format(embeddings.most_similar_to_given('killer', ['black', 'white', 'asian'])))\n",
    "# Was bleibt vom Mann, wenn die Intelligenz abgezogen wird?\n",
    "print('Mann ohne Intelligenz: {}'.format(embeddings.most_similar(positive=['man'], negative=['intelligence'])[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Von Wörtern zu Sätzen\n",
    "Nachdem wir uns mit der semantischen Repräsentation von Wörtern beschäftigt haben, wollen wir jetzt zu Sätzen übergehen. Wie bereits gesagt, wollen wir die Ähnlichkeit von Satzpaaren bestimmen. Dazu brauchen wir einen Testdatensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.1: Testdaten einlesen\n",
    "Die Datei ```sts-train.csv``` enthält 5749 Satzpaare, deren Ähnlichkeit jeweils mit einem Score zwischen 0 (völlig unähnlich) und 5 (völlig ähnlich) bewertet wurde. Lest diese Datei in einen Pandas-Dataframe ein.\n",
    "\n",
    "Zu beachten sind die folgenden Punkte:\n",
    "* Bei den CSV-Dateien handelt es sich eigentlich strenggenommen um TSV-Dateien, d.h. die einzelnen Spalten sind durch Tab getrennt.\n",
    "* Einige Zeilen enthalten unvollständige Quotes, die zu Fehlern beim Einlesen führen können. Diese sollen ignoriert werden. (```quoting=csv.QUOTE_NONE```)\n",
    "* Zitat aus der zum Datenset gehörigen README: \n",
    ">Each file is encoded in utf-8 (a superset of ASCII), and has the following tab separated fields:  \n",
    ">__genre filename year score sentence1 sentence2__  \n",
    ">optionally there might be some license-related fields after sentence2.\n",
    "\n",
    "Neben den uns interessierenden Spalten \"score\", \"sentence1\" und \"sentence2\" sind also noch weitere Spalten enthalten. Wir können den Parameter ```usecols``` verwenden, um die Indizes der uns interessierenden Spalten anzugeben (Zählung beginnt bei 1). Für die Spalten vergeben wir die Namen \"score\", \"sentence1\" und \"sentence2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "testdata =  pd.read_csv('./data/sts-train.csv', sep='\\t', quoting=csv.QUOTE_NONE, usecols=[4, 5, 6], names = [\"score\", \"sentence1\", \"sentence2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.800</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.800</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.600</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.250</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.250</td>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.500</td>\n",
       "      <td>A man is smoking.</td>\n",
       "      <td>A man is skating.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.600</td>\n",
       "      <td>The man is playing the piano.</td>\n",
       "      <td>The man is playing the guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.200</td>\n",
       "      <td>A man is playing on a guitar and singing.</td>\n",
       "      <td>A woman is playing an acoustic guitar and sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.000</td>\n",
       "      <td>A person is throwing a cat on to the ceiling.</td>\n",
       "      <td>A person throws a cat on the ceiling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.200</td>\n",
       "      <td>The man hit the other man with a stick.</td>\n",
       "      <td>The man spanked the other man with a stick.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.600</td>\n",
       "      <td>A woman picks up and holds a baby kangaroo.</td>\n",
       "      <td>A woman picks up and holds a baby kangaroo in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.867</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>A man is playing a bamboo flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.667</td>\n",
       "      <td>A person is folding a piece of paper.</td>\n",
       "      <td>Someone is folding a piece of paper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.667</td>\n",
       "      <td>A man is running on the road.</td>\n",
       "      <td>A panda dog is running on the road.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.750</td>\n",
       "      <td>A dog is trying to get bacon off his back.</td>\n",
       "      <td>A dog is trying to eat the bacon on its back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.000</td>\n",
       "      <td>The polar bear is sliding on the snow.</td>\n",
       "      <td>A polar bear is sliding across the snow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.500</td>\n",
       "      <td>A woman is writing.</td>\n",
       "      <td>A woman is swimming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.800</td>\n",
       "      <td>A cat is rubbing against baby's face.</td>\n",
       "      <td>A cat is rubbing against a baby.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.000</td>\n",
       "      <td>The man is riding a horse.</td>\n",
       "      <td>A man is riding on a horse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.200</td>\n",
       "      <td>A man pours oil into a pot.</td>\n",
       "      <td>A man pours wine in a pot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.800</td>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>A girl is playing a guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.600</td>\n",
       "      <td>A panda is sliding down a slide.</td>\n",
       "      <td>A panda slides down a slide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.000</td>\n",
       "      <td>A woman is eating something.</td>\n",
       "      <td>A woman is eating meat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.000</td>\n",
       "      <td>A woman peels a potato.</td>\n",
       "      <td>A woman is peeling a potato.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.800</td>\n",
       "      <td>The boy fell off his bike.</td>\n",
       "      <td>A boy falls off his bike.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.000</td>\n",
       "      <td>The woman is playing the flute.</td>\n",
       "      <td>A woman is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.200</td>\n",
       "      <td>A rabbit is running from an eagle.</td>\n",
       "      <td>A hare is running from a eagle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.200</td>\n",
       "      <td>The woman is frying a breaded pork chop.</td>\n",
       "      <td>A woman is cooking a breaded pork chop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.000</td>\n",
       "      <td>A girl is flying a kite.</td>\n",
       "      <td>A girl running is flying a kite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>1.000</td>\n",
       "      <td>Boko Haram disrupts Nigerian elections</td>\n",
       "      <td>Press awash with Nigeria\\'s general elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>1.000</td>\n",
       "      <td>South Korea regrets North's refusal of talks</td>\n",
       "      <td>South Korea reports 14th Mers death, 12 new cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Plane Crash Site Search Called off for the Night</td>\n",
       "      <td>Tour De France Pack Sets off for Stage 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5722</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Obama queries Turnbull over China port deal</td>\n",
       "      <td>Spanish bulls gore seven to death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>0.000</td>\n",
       "      <td>China's generous 1MDB bid seen reaping it big ...</td>\n",
       "      <td>China keeps an eye on visiting tiger freed by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Hezbollah targets Qaeda gathering along border...</td>\n",
       "      <td>Belarus tightens security along the border wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>1.000</td>\n",
       "      <td>South Korea reports seven new MERS cases</td>\n",
       "      <td>South Korean woman in contact with MERS patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Obama calls for international front against IS</td>\n",
       "      <td>Obama vows to save Iraqis stranded on mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>1.000</td>\n",
       "      <td>U.S. military says it conducts airstrikes agai...</td>\n",
       "      <td>Observatory says Russian air strikes kill 45 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Who abandoned 3 Boeing planes at the airport?</td>\n",
       "      <td>Police abandon posts in Lesotho, fear for lives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>2.000</td>\n",
       "      <td>North Korea to put US citizen on trial</td>\n",
       "      <td>N Korea hit with new US sanctions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5730</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Hong Kong officials resume work as protests thin</td>\n",
       "      <td>Hong Kong student leaders mull protest retreat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5731</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Ebola UK: NHS staff 'panicked' after suspected...</td>\n",
       "      <td>UK says investigating 2 suspected MERS cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5732</th>\n",
       "      <td>1.000</td>\n",
       "      <td>UK Leader Signals Support for Airstrikes on IS...</td>\n",
       "      <td>Why Cyprus backs Cameron's push for UK air str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>0.000</td>\n",
       "      <td>N. Korea agrees to talks with South</td>\n",
       "      <td>Johor crowns its fifth Sultan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Iran, a large important partner for Russia: Novak</td>\n",
       "      <td>Iran reiterates support for Iraq, Syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Taiwan's president charters plane to pay respe...</td>\n",
       "      <td>Poland president loses to challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Two Palestinians die in attacks on Israelis ah...</td>\n",
       "      <td>Palestinian official slams Israel's stone-thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Thomas Cook accused of putting costs before cu...</td>\n",
       "      <td>University of Florida frat accused of spitting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Narendra Modi 'photoshopped' image of Chennai ...</td>\n",
       "      <td>PM Narendra Modi speaks of non-discrimination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5739</th>\n",
       "      <td>1.000</td>\n",
       "      <td>UN chief welcomes peaceful presidential electi...</td>\n",
       "      <td>UN chief condemns attack against peacekeepers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5740</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Pakistan executions draw international ire</td>\n",
       "      <td>Pakistan protests ceasefire violations by India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Two Australians killed in Kenya after bus cras...</td>\n",
       "      <td>Permalink to Two killed in Germany train accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5742</th>\n",
       "      <td>2.000</td>\n",
       "      <td>France shuts mosque, arrests man in crackdown ...</td>\n",
       "      <td>Security tightened at New Delhi churches after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5743</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Russian plane reportedly crashes in Egypt</td>\n",
       "      <td>Pilot killed as US jet crashes in England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Severe Gales As Storm Clodagh Hits Britain</td>\n",
       "      <td>Merkel pledges NATO solidarity with Latvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5745</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Dozens of Egyptians hostages taken by Libyan t...</td>\n",
       "      <td>Egyptian boat crash death toll rises as more b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>0.000</td>\n",
       "      <td>President heading to Bahrain</td>\n",
       "      <td>President Xi: China to continue help to fight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>0.000</td>\n",
       "      <td>China, India vow to further bilateral ties</td>\n",
       "      <td>China Scrambles to Reassure Jittery Stock Traders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5748</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Putin spokesman: Doping charges appear unfounded</td>\n",
       "      <td>The Latest on Severe Weather: 1 Dead in Texas ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5749 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                          sentence1  \\\n",
       "0     5.000                             A plane is taking off.   \n",
       "1     3.800                    A man is playing a large flute.   \n",
       "2     3.800      A man is spreading shreded cheese on a pizza.   \n",
       "3     2.600                       Three men are playing chess.   \n",
       "4     4.250                        A man is playing the cello.   \n",
       "5     4.250                             Some men are fighting.   \n",
       "6     0.500                                  A man is smoking.   \n",
       "7     1.600                      The man is playing the piano.   \n",
       "8     2.200          A man is playing on a guitar and singing.   \n",
       "9     5.000      A person is throwing a cat on to the ceiling.   \n",
       "10    4.200            The man hit the other man with a stick.   \n",
       "11    4.600        A woman picks up and holds a baby kangaroo.   \n",
       "12    3.867                          A man is playing a flute.   \n",
       "13    4.667              A person is folding a piece of paper.   \n",
       "14    1.667                      A man is running on the road.   \n",
       "15    3.750         A dog is trying to get bacon off his back.   \n",
       "16    5.000             The polar bear is sliding on the snow.   \n",
       "17    0.500                                A woman is writing.   \n",
       "18    3.800              A cat is rubbing against baby's face.   \n",
       "19    5.000                         The man is riding a horse.   \n",
       "20    3.200                        A man pours oil into a pot.   \n",
       "21    2.800                         A man is playing a guitar.   \n",
       "22    4.600                   A panda is sliding down a slide.   \n",
       "23    3.000                       A woman is eating something.   \n",
       "24    5.000                            A woman peels a potato.   \n",
       "25    4.800                         The boy fell off his bike.   \n",
       "26    5.000                    The woman is playing the flute.   \n",
       "27    4.200                 A rabbit is running from an eagle.   \n",
       "28    4.200           The woman is frying a breaded pork chop.   \n",
       "29    4.000                           A girl is flying a kite.   \n",
       "...     ...                                                ...   \n",
       "5719  1.000             Boko Haram disrupts Nigerian elections   \n",
       "5720  1.000       South Korea regrets North's refusal of talks   \n",
       "5721  0.000   Plane Crash Site Search Called off for the Night   \n",
       "5722  0.000        Obama queries Turnbull over China port deal   \n",
       "5723  0.000  China's generous 1MDB bid seen reaping it big ...   \n",
       "5724  0.000  Hezbollah targets Qaeda gathering along border...   \n",
       "5725  1.000           South Korea reports seven new MERS cases   \n",
       "5726  0.000     Obama calls for international front against IS   \n",
       "5727  1.000  U.S. military says it conducts airstrikes agai...   \n",
       "5728  0.000      Who abandoned 3 Boeing planes at the airport?   \n",
       "5729  2.000             North Korea to put US citizen on trial   \n",
       "5730  2.000   Hong Kong officials resume work as protests thin   \n",
       "5731  0.000  Ebola UK: NHS staff 'panicked' after suspected...   \n",
       "5732  1.000  UK Leader Signals Support for Airstrikes on IS...   \n",
       "5733  0.000                N. Korea agrees to talks with South   \n",
       "5734  0.000  Iran, a large important partner for Russia: Novak   \n",
       "5735  0.000  Taiwan's president charters plane to pay respe...   \n",
       "5736  0.000  Two Palestinians die in attacks on Israelis ah...   \n",
       "5737  0.000  Thomas Cook accused of putting costs before cu...   \n",
       "5738  0.000  Narendra Modi 'photoshopped' image of Chennai ...   \n",
       "5739  1.000  UN chief welcomes peaceful presidential electi...   \n",
       "5740  0.000         Pakistan executions draw international ire   \n",
       "5741  0.000  Two Australians killed in Kenya after bus cras...   \n",
       "5742  2.000  France shuts mosque, arrests man in crackdown ...   \n",
       "5743  2.000          Russian plane reportedly crashes in Egypt   \n",
       "5744  0.000         Severe Gales As Storm Clodagh Hits Britain   \n",
       "5745  0.000  Dozens of Egyptians hostages taken by Libyan t...   \n",
       "5746  0.000                       President heading to Bahrain   \n",
       "5747  0.000         China, India vow to further bilateral ties   \n",
       "5748  0.000   Putin spokesman: Doping charges appear unfounded   \n",
       "\n",
       "                                              sentence2  \n",
       "0                           An air plane is taking off.  \n",
       "1                             A man is playing a flute.  \n",
       "2     A man is spreading shredded cheese on an uncoo...  \n",
       "3                            Two men are playing chess.  \n",
       "4                    A man seated is playing the cello.  \n",
       "5                                 Two men are fighting.  \n",
       "6                                     A man is skating.  \n",
       "7                        The man is playing the guitar.  \n",
       "8     A woman is playing an acoustic guitar and sing...  \n",
       "9                 A person throws a cat on the ceiling.  \n",
       "10          The man spanked the other man with a stick.  \n",
       "11    A woman picks up and holds a baby kangaroo in ...  \n",
       "12                     A man is playing a bamboo flute.  \n",
       "13                 Someone is folding a piece of paper.  \n",
       "14                  A panda dog is running on the road.  \n",
       "15        A dog is trying to eat the bacon on its back.  \n",
       "16             A polar bear is sliding across the snow.  \n",
       "17                                 A woman is swimming.  \n",
       "18                     A cat is rubbing against a baby.  \n",
       "19                          A man is riding on a horse.  \n",
       "20                           A man pours wine in a pot.  \n",
       "21                          A girl is playing a guitar.  \n",
       "22                         A panda slides down a slide.  \n",
       "23                              A woman is eating meat.  \n",
       "24                         A woman is peeling a potato.  \n",
       "25                            A boy falls off his bike.  \n",
       "26                          A woman is playing a flute.  \n",
       "27                      A hare is running from a eagle.  \n",
       "28              A woman is cooking a breaded pork chop.  \n",
       "29                     A girl running is flying a kite.  \n",
       "...                                                 ...  \n",
       "5719      Press awash with Nigeria\\'s general elections  \n",
       "5720  South Korea reports 14th Mers death, 12 new cases  \n",
       "5721          Tour De France Pack Sets off for Stage 15  \n",
       "5722                  Spanish bulls gore seven to death  \n",
       "5723  China keeps an eye on visiting tiger freed by ...  \n",
       "5724  Belarus tightens security along the border wit...  \n",
       "5725  South Korean woman in contact with MERS patien...  \n",
       "5726     Obama vows to save Iraqis stranded on mountain  \n",
       "5727  Observatory says Russian air strikes kill 45 i...  \n",
       "5728    Police abandon posts in Lesotho, fear for lives  \n",
       "5729                  N Korea hit with new US sanctions  \n",
       "5730     Hong Kong student leaders mull protest retreat  \n",
       "5731       UK says investigating 2 suspected MERS cases  \n",
       "5732  Why Cyprus backs Cameron's push for UK air str...  \n",
       "5733                      Johor crowns its fifth Sultan  \n",
       "5734            Iran reiterates support for Iraq, Syria  \n",
       "5735               Poland president loses to challenger  \n",
       "5736  Palestinian official slams Israel's stone-thro...  \n",
       "5737  University of Florida frat accused of spitting...  \n",
       "5738  PM Narendra Modi speaks of non-discrimination ...  \n",
       "5739  UN chief condemns attack against peacekeepers ...  \n",
       "5740    Pakistan protests ceasefire violations by India  \n",
       "5741  Permalink to Two killed in Germany train accident  \n",
       "5742  Security tightened at New Delhi churches after...  \n",
       "5743          Pilot killed as US jet crashes in England  \n",
       "5744         Merkel pledges NATO solidarity with Latvia  \n",
       "5745  Egyptian boat crash death toll rises as more b...  \n",
       "5746  President Xi: China to continue help to fight ...  \n",
       "5747  China Scrambles to Reassure Jittery Stock Traders  \n",
       "5748  The Latest on Severe Weather: 1 Dead in Texas ...  \n",
       "\n",
       "[5749 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.2: Semantische Repräsentation von Sätzen\n",
    "Genau wie für Wörter wollen wir auch die Semantik von Sätzen in einem Vektor der Dimension 300 abbilden. Dazu wählen wir einen recht einfachen Ansatz und mitteln die Vektoren der Wörter, aus denen der Satz besteht.\n",
    "\n",
    "Schreibt eine Funktion, die einen Satz entgegennimmt und einen Vektor mit den gemittelten semantischen Embeddings zurückgibt. Wird ein leerer String übergeben oder enthält der übergebene Satz nur Wörter, für die keine Embeddings vorhanden sind, soll ein Nullvektor der Länge 300 zurückgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "def process_word(word):\n",
    "    word = word.lower()\n",
    "    word = word.strip(punctuation)\n",
    "    return word\n",
    "\n",
    "def sentence_to_vec(sentence):\n",
    "    sentence = [process_word(word) for word in sentence.split() if process_word(word) in embeddings.vocab]\n",
    "    vectors = [embeddings[word] for word in sentence]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec_tests():\n",
    "    if (np.zeros(300) != sentence_to_vec('')).any():\n",
    "        return \"Bei leerer Eingabe soll ein Nullvektor zurückgegeben werden\"\n",
    "    if (np.zeros(300) != sentence_to_vec('thereisnosuchword')).any():\n",
    "        return \"Wenn keine Embeddings gefunden werden, soll ein Nullvektor zurückgegeben werden.\"\n",
    "    if (embeddings['word'] != sentence_to_vec('word')).any():\n",
    "        return \"Da stimmt was nicht.\"\n",
    "    if ((embeddings['cat'] + embeddings['dog']) / 2 != sentence_to_vec('cat dog')).any():\n",
    "        return \"Die Funktion soll den Durchschnittswert der Vektoren berechnen.\"\n",
    "    if (embeddings['word'] != sentence_to_vec('thereisnosuchword word')).any():\n",
    "        return \"Wörter, deren Embedding nicht bekannt ist, sollen nicht berücksichtigt werden.\"\n",
    "    return \"Grundlegende Tests bestanden.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grundlegende Tests bestanden.\n"
     ]
    }
   ],
   "source": [
    "print(sentence_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.3: Ähnlichkeitsberechnung\n",
    "Um die Ähnlichkeit zweier Vektoren zu bestimmen, wird häufig die [Kosinusähnlichkeit](https://en.wikipedia.org/wiki/Cosine_similarity) verwendet. Weil wir die von uns berechneten Ähnlichkeitswerte aber mit denen der Testdaten vergleichen können wollen, müssen wir den Wertebereich so anpassen, dass wir Werte zwischen 0 und 5 erhalten. Dabei treffen wir die Annahme, dass eine Kosinusähnlichkeit kleiner-gleich 0 als unähnlich (Wert ```0```) zu werten ist.\n",
    "\n",
    "Schreibt eine Funktion, die zwei Sätze entgegennimmt und Ähnlichkeitswerte im Bereich ```[0, 5]``` zurückgibt, basierend auf der Kosinusähnlichkeit der semantischen Vektoren der Sätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def compute_similarity_score(sentence_1, sentence_2):\n",
    "    cos_similarity = cosine_similarity(sentence_to_vec(sentence_1).reshape(1, -1), sentence_to_vec(sentence_2).reshape(1, -1))\n",
    "    our_similarity = max(0.0, cos_similarity[0][0]) * 5\n",
    "    return our_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grundlegende Tests bestanden.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def compute_similarity_score_test():\n",
    "    if not (math.isclose(compute_similarity_score(\"hot dog\", \"hot dog\"), 5.0)):\n",
    "        return \"Identische Sätze sollten einen Ähnlichkeitswert von 5 haben.\"\n",
    "    if not (math.isclose(compute_similarity_score(\"dog hot\", \"hot dog\"), 5.0)):\n",
    "        return \"Sätze, die sich nur in der Anordnung der Wörter unterscheiden, sollten einen Ähnlichkeitswert von 5 haben.\"\n",
    "    if not (math.isclose(compute_similarity_score(\"nosuchword\", \"word\"), 0.0)):\n",
    "        return \"Der Vergleich eines unbekannten Wortes mit einem bekannten, sollte zu einem Ähnlichkeitswert von 0 führen.\"\n",
    "    if (math.isclose(compute_similarity_score(\"bread\", \"cake\"), 0.0)):\n",
    "        return \"Verwandte Worte sollten einen Ähnlichkeitswert größer Null aufweisen.\"\n",
    "    return \"Grundlegende Tests bestanden.\"\n",
    "\n",
    "compute_similarity_score_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Validierung\n",
    "Im letzten Schritt wollen wir bestimmen, wie gut unsere Ähnlichkeitsberechnung funktioniert, d.h. wie stark die von uns berechneten Ähnlichkeitswerte für die Satzpaare aus dem Testdatensatz mit den erwarteten Werten übereinstimmen.\n",
    "\n",
    "Dazu verwenden wir den [Korrelationskoeffizienten](https://de.wikipedia.org/wiki/Korrelationskoeffizient), der uns in ```scipy``` als [```stats.pearsonr()```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) zur Verfügung steht.\n",
    "\n",
    "Schreibt eine Funktion, die zwei Listen mit Ähnlichkeitswerten entgegennimmt und für diese den Pearsonscore berechnet. Wendet die Funktion auf die für die Testdaten erwarteten und die von uns berechneten Ähnlichkeitsscores an und interpretiert das Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "def calculate_pearson_score(expected, actual):\n",
    "    return stats.pearsonr(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6821517528724196, 0.0)\n"
     ]
    }
   ],
   "source": [
    "expected_scores = testdata['score'].to_numpy()\n",
    "our_scores = testdata.apply(lambda row: compute_similarity_score(row['sentence1'], row['sentence2']), axis=1)\n",
    "    \n",
    "print(calculate_pearson_score(expected_scores, our_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
